{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rethinking-Network-Pruning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lehduong/GINP/blob/testing/Rethinking_Network_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXhkT8KV3BAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0egBUuqt53",
        "colab_type": "text"
      },
      "source": [
        "# Gifts from Iterative Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP2Uk6Dqqz9H",
        "colab_type": "text"
      },
      "source": [
        "## 1. Prepare\n",
        "\n",
        "Clone github repo and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwCfi3fo3C0c",
        "colab_type": "code",
        "outputId": "eeeb2f0b-4f46-466d-be6e-cf98449b68c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/lehduong/ginp -b testing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ginp'...\n",
            "remote: Enumerating objects: 469, done.\u001b[K\n",
            "remote: Counting objects: 100% (469/469), done.\u001b[K\n",
            "remote: Compressing objects: 100% (204/204), done.\u001b[K\n",
            "remote: Total 469 (delta 289), reused 438 (delta 259), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (469/469), 1.12 MiB | 2.72 MiB/s, done.\n",
            "Resolving deltas: 100% (289/289), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jC27n-C3aOu",
        "colab_type": "code",
        "outputId": "fd6f2523-19ac-43ea-ae77-fc81547916e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/ginp/cifar/filter_pruning/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ginp/cifar/filter_pruning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2IMJJ9o5r9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ptflops"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3blTvgBq4OE",
        "colab_type": "text"
      },
      "source": [
        "## 2. Train and Evaluate Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoXJg95QzIwl",
        "colab_type": "text"
      },
      "source": [
        "### 1. Training from scratch \n",
        "\n",
        "Training the baseline model from scratch. Note that, for training from scratch, I have only tested on *cifar/**filter_pruning**/train.py*. \n",
        "\n",
        "Change the **--arch** option to desired architecture. It should be one of: resnet56, resnet110, wrn_16_10, vgg16, preresnet110. Read *cifar/xxxx_pruning/models/__init__.py* for more details.\n",
        "\n",
        "+ For resnet56, 110, preresnet110, vgg: $training\\_epochs = 300$, $schedule=[150,225]$, $gamma=0.1$, $lr=0.1$\n",
        "\n",
        "+ For wrn_16_10: $training\\_epochs = 200$, $schedule=[60,120, 160]$, $gamma=0.2$, $lr=0.1$\n",
        "\n",
        "Currently, I used $weight\\_decay=0.0001$ for all models thought original paper of wideresnet suggested using $0.0005$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF9uakuT3g98",
        "colab_type": "code",
        "outputId": "f04630fd-60d6-483a-c21e-eb530259d8b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py -a wrn_16_8 -d cifar100 --epochs 200 --schedule 60 120 160 --gamma 0.2 --wd 1e-4 --save checkpoints"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing dataset cifar100\n",
            "Files already downloaded and verified\n",
            "==> creating model 'wrn_16_8'\n",
            "    Total params: 11.01M\n",
            "\n",
            "Epoch: [1 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.100s | Total: 0:00:38 | ETA: 0:00:01 | Loss: 3.8460 | top1:  10.4300 | top5:  31.5840\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.032s | Total: 0:00:03 | ETA: 0:00:01 | Loss: 3.6172 | top1:  14.5700 | top5:  41.5400\n",
            "\n",
            "Epoch: [2 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 3.0114 | top1:  24.3120 | top5:  54.6980\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 2.8561 | top1:  27.0800 | top5:  59.6800\n",
            "\n",
            "Epoch: [3 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 2.4084 | top1:  36.4100 | top5:  69.3980\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 2.4437 | top1:  36.3800 | top5:  69.7000\n",
            "\n",
            "Epoch: [4 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 2.0074 | top1:  45.1060 | top5:  77.7620\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 2.1340 | top1:  43.9800 | top5:  76.9800\n",
            "\n",
            "Epoch: [5 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 1.7364 | top1:  51.4260 | top5:  82.7320\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.9620 | top1:  48.0600 | top5:  78.7000\n",
            "\n",
            "Epoch: [6 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 1.5337 | top1:  56.3680 | top5:  86.1780\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.9290 | top1:  49.5100 | top5:  80.7300\n",
            "\n",
            "Epoch: [7 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 1.3866 | top1:  60.2040 | top5:  88.1860\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.8270 | top1:  52.5100 | top5:  82.1200\n",
            "\n",
            "Epoch: [8 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 1.2596 | top1:  63.2880 | top5:  90.2080\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6710 | top1:  55.0200 | top5:  84.5000\n",
            "\n",
            "Epoch: [9 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 1.1464 | top1:  66.5760 | top5:  91.5240\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.5665 | top1:  58.0200 | top5:  85.8300\n",
            "\n",
            "Epoch: [10 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 1.0577 | top1:  68.8540 | top5:  92.7060\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6081 | top1:  57.0800 | top5:  85.4300\n",
            "\n",
            "Epoch: [11 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.9808 | top1:  70.7220 | top5:  93.5400\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.5833 | top1:  58.3800 | top5:  86.4200\n",
            "\n",
            "Epoch: [12 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.9037 | top1:  72.7200 | top5:  94.5380\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.4562 | top1:  61.1100 | top5:  87.5400\n",
            "\n",
            "Epoch: [13 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.8371 | top1:  74.7540 | top5:  95.2120\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.4951 | top1:  61.4300 | top5:  87.8500\n",
            "\n",
            "Epoch: [14 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.7810 | top1:  76.2760 | top5:  95.8040\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.4230 | top1:  62.4100 | top5:  88.8400\n",
            "\n",
            "Epoch: [15 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.7271 | top1:  77.7880 | top5:  96.4360\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.3247 | top1:  65.0400 | top5:  89.8300\n",
            "\n",
            "Epoch: [16 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.6725 | top1:  79.2620 | top5:  96.8400\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.3976 | top1:  64.4100 | top5:  89.1800\n",
            "\n",
            "Epoch: [17 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.6386 | top1:  80.1160 | top5:  97.1840\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.2936 | top1:  66.5000 | top5:  90.1800\n",
            "\n",
            "Epoch: [18 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.5900 | top1:  81.7960 | top5:  97.6080\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.4223 | top1:  64.9100 | top5:  89.9500\n",
            "\n",
            "Epoch: [19 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.5539 | top1:  82.5140 | top5:  97.9760\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.5348 | top1:  63.2000 | top5:  88.5000\n",
            "\n",
            "Epoch: [20 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.5222 | top1:  83.3940 | top5:  98.2120\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.4780 | top1:  65.5100 | top5:  89.4200\n",
            "\n",
            "Epoch: [21 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.4898 | top1:  84.4420 | top5:  98.4440\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.5737 | top1:  62.9900 | top5:  88.3100\n",
            "\n",
            "Epoch: [22 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.4591 | top1:  85.3040 | top5:  98.6480\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6041 | top1:  64.0100 | top5:  88.4500\n",
            "\n",
            "Epoch: [23 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.4297 | top1:  86.3520 | top5:  98.7380\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.4556 | top1:  67.3200 | top5:  90.1300\n",
            "\n",
            "Epoch: [24 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.4167 | top1:  86.7700 | top5:  98.9380\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6154 | top1:  64.7900 | top5:  88.8400\n",
            "\n",
            "Epoch: [25 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.4024 | top1:  87.2680 | top5:  98.9900\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.4604 | top1:  66.2400 | top5:  89.9400\n",
            "\n",
            "Epoch: [26 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.3774 | top1:  87.8460 | top5:  99.0940\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.5113 | top1:  65.2000 | top5:  89.5100\n",
            "\n",
            "Epoch: [27 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.3520 | top1:  88.8020 | top5:  99.2420\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.4548 | top1:  67.0200 | top5:  89.8900\n",
            "\n",
            "Epoch: [28 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.3403 | top1:  89.1640 | top5:  99.2860\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.5129 | top1:  65.6500 | top5:  89.8500\n",
            "\n",
            "Epoch: [29 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.3247 | top1:  89.3800 | top5:  99.4100\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.5908 | top1:  65.6200 | top5:  89.7600\n",
            "\n",
            "Epoch: [30 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.3016 | top1:  90.1280 | top5:  99.5260\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7072 | top1:  63.5100 | top5:  88.1300\n",
            "\n",
            "Epoch: [31 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.3007 | top1:  90.1560 | top5:  99.5160\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7027 | top1:  65.4100 | top5:  88.7700\n",
            "\n",
            "Epoch: [32 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2778 | top1:  91.1440 | top5:  99.5660\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6817 | top1:  65.6900 | top5:  89.3800\n",
            "\n",
            "Epoch: [33 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2725 | top1:  91.2240 | top5:  99.6400\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.5501 | top1:  66.3600 | top5:  89.7400\n",
            "\n",
            "Epoch: [34 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2745 | top1:  91.1760 | top5:  99.6220\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6362 | top1:  64.5800 | top5:  89.1400\n",
            "\n",
            "Epoch: [35 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2619 | top1:  91.5520 | top5:  99.6520\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6799 | top1:  66.2500 | top5:  89.4300\n",
            "\n",
            "Epoch: [36 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2564 | top1:  91.6380 | top5:  99.6520\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6154 | top1:  65.8400 | top5:  89.0200\n",
            "\n",
            "Epoch: [37 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2538 | top1:  91.8200 | top5:  99.6600\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7072 | top1:  65.2700 | top5:  88.8500\n",
            "\n",
            "Epoch: [38 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2424 | top1:  92.2480 | top5:  99.6860\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6951 | top1:  64.2500 | top5:  88.7500\n",
            "\n",
            "Epoch: [39 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2352 | top1:  92.4380 | top5:  99.7240\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7610 | top1:  65.2700 | top5:  89.1000\n",
            "\n",
            "Epoch: [40 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2350 | top1:  92.3960 | top5:  99.7260\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7101 | top1:  65.6900 | top5:  88.8700\n",
            "\n",
            "Epoch: [41 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2298 | top1:  92.6460 | top5:  99.7400\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7354 | top1:  65.8100 | top5:  89.1900\n",
            "\n",
            "Epoch: [42 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2416 | top1:  92.2140 | top5:  99.7620\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.8626 | top1:  63.3100 | top5:  87.7500\n",
            "\n",
            "Epoch: [43 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2246 | top1:  92.8740 | top5:  99.7240\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6190 | top1:  66.5500 | top5:  89.4000\n",
            "\n",
            "Epoch: [44 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2100 | top1:  93.2920 | top5:  99.7920\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7308 | top1:  65.8300 | top5:  88.9900\n",
            "\n",
            "Epoch: [45 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2159 | top1:  93.2380 | top5:  99.7740\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6965 | top1:  66.1600 | top5:  88.7800\n",
            "\n",
            "Epoch: [46 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2236 | top1:  92.8040 | top5:  99.7840\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7003 | top1:  66.6300 | top5:  89.6300\n",
            "\n",
            "Epoch: [47 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2030 | top1:  93.5340 | top5:  99.7860\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7829 | top1:  65.2500 | top5:  88.4200\n",
            "\n",
            "Epoch: [48 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.1997 | top1:  93.6500 | top5:  99.7900\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.4932 | top1:  68.3200 | top5:  90.8000\n",
            "\n",
            "Epoch: [49 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.1927 | top1:  93.9700 | top5:  99.8240\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7214 | top1:  66.1900 | top5:  89.2200\n",
            "\n",
            "Epoch: [50 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2129 | top1:  93.1900 | top5:  99.7860\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7711 | top1:  65.2400 | top5:  88.0800\n",
            "\n",
            "Epoch: [51 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2115 | top1:  93.3280 | top5:  99.7480\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6018 | top1:  67.8400 | top5:  90.2600\n",
            "\n",
            "Epoch: [52 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.1934 | top1:  93.9120 | top5:  99.8240\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.8254 | top1:  65.3600 | top5:  88.6000\n",
            "\n",
            "Epoch: [53 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.1996 | top1:  93.6080 | top5:  99.8260\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6291 | top1:  66.8100 | top5:  89.4400\n",
            "\n",
            "Epoch: [54 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2218 | top1:  92.9640 | top5:  99.7480\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7630 | top1:  66.1700 | top5:  88.6100\n",
            "\n",
            "Epoch: [55 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.1961 | top1:  93.8600 | top5:  99.8260\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.8646 | top1:  65.1200 | top5:  88.6600\n",
            "\n",
            "Epoch: [56 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.2048 | top1:  93.5180 | top5:  99.7520\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.6294 | top1:  67.4200 | top5:  89.8400\n",
            "\n",
            "Epoch: [57 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.1887 | top1:  94.0560 | top5:  99.8200\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.8479 | top1:  65.1800 | top5:  88.9600\n",
            "\n",
            "Epoch: [58 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.1804 | top1:  94.3180 | top5:  99.8540\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.7041 | top1:  66.5400 | top5:  89.3100\n",
            "\n",
            "Epoch: [59 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.1903 | top1:  94.0220 | top5:  99.8100\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.8598 | top1:  64.8900 | top5:  87.8100\n",
            "\n",
            "Epoch: [60 | 200] LR: 0.100000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.1890 | top1:  94.0740 | top5:  99.8260\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.5285 | top1:  68.1500 | top5:  89.9100\n",
            "\n",
            "Epoch: [61 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0516 | top1:  98.8020 | top5:  99.9920\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.1178 | top1:  75.5600 | top5:  93.6200\n",
            "\n",
            "Epoch: [62 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0235 | top1:  99.6480 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.1180 | top1:  75.8000 | top5:  93.6600\n",
            "\n",
            "Epoch: [63 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0173 | top1:  99.7900 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.1106 | top1:  76.0000 | top5:  93.7500\n",
            "\n",
            "Epoch: [64 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0144 | top1:  99.8460 | top5:  99.9980\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.1170 | top1:  76.1500 | top5:  93.7200\n",
            "\n",
            "Epoch: [65 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0128 | top1:  99.8760 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.1038 | top1:  76.3900 | top5:  93.7400\n",
            "\n",
            "Epoch: [66 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0116 | top1:  99.8940 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.1070 | top1:  76.2500 | top5:  93.9400\n",
            "\n",
            "Epoch: [67 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0102 | top1:  99.9260 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.1001 | top1:  76.3700 | top5:  93.9300\n",
            "\n",
            "Epoch: [68 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0103 | top1:  99.8920 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0900 | top1:  76.5500 | top5:  93.8700\n",
            "\n",
            "Epoch: [69 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0096 | top1:  99.9260 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0957 | top1:  76.4700 | top5:  93.8300\n",
            "\n",
            "Epoch: [70 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0090 | top1:  99.9400 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0905 | top1:  76.6000 | top5:  93.8400\n",
            "\n",
            "Epoch: [71 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0085 | top1:  99.9500 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0831 | top1:  76.6100 | top5:  93.7400\n",
            "\n",
            "Epoch: [72 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0080 | top1:  99.9380 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0866 | top1:  76.3800 | top5:  93.8400\n",
            "\n",
            "Epoch: [73 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0081 | top1:  99.9360 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0874 | top1:  76.6500 | top5:  93.7900\n",
            "\n",
            "Epoch: [74 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0080 | top1:  99.9460 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0790 | top1:  76.8600 | top5:  93.7400\n",
            "\n",
            "Epoch: [75 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0074 | top1:  99.9520 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0737 | top1:  76.7700 | top5:  93.7800\n",
            "\n",
            "Epoch: [76 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0071 | top1:  99.9640 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0603 | top1:  76.6500 | top5:  93.9400\n",
            "\n",
            "Epoch: [77 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0070 | top1:  99.9500 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0517 | top1:  76.7600 | top5:  93.8600\n",
            "\n",
            "Epoch: [78 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0073 | top1:  99.9540 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0570 | top1:  76.7900 | top5:  93.8400\n",
            "\n",
            "Epoch: [79 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0068 | top1:  99.9700 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0519 | top1:  76.7900 | top5:  93.8500\n",
            "\n",
            "Epoch: [80 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0067 | top1:  99.9600 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0486 | top1:  76.7800 | top5:  93.9800\n",
            "\n",
            "Epoch: [81 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.096s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0067 | top1:  99.9560 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0375 | top1:  76.7500 | top5:  93.8600\n",
            "\n",
            "Epoch: [82 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0067 | top1:  99.9640 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.004s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0383 | top1:  76.9200 | top5:  93.8200\n",
            "\n",
            "Epoch: [83 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0066 | top1:  99.9720 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0349 | top1:  76.8100 | top5:  93.8200\n",
            "\n",
            "Epoch: [84 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0065 | top1:  99.9560 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0321 | top1:  76.7600 | top5:  93.8800\n",
            "\n",
            "Epoch: [85 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0066 | top1:  99.9700 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0290 | top1:  76.8300 | top5:  93.9000\n",
            "\n",
            "Epoch: [86 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0070 | top1:  99.9540 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0235 | top1:  76.9500 | top5:  93.9400\n",
            "\n",
            "Epoch: [87 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0066 | top1:  99.9540 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0211 | top1:  77.0900 | top5:  93.8000\n",
            "\n",
            "Epoch: [88 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0068 | top1:  99.9620 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0236 | top1:  76.8000 | top5:  93.9300\n",
            "\n",
            "Epoch: [89 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0066 | top1:  99.9660 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0160 | top1:  77.0100 | top5:  93.8500\n",
            "\n",
            "Epoch: [90 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0066 | top1:  99.9600 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0162 | top1:  76.8500 | top5:  93.8300\n",
            "\n",
            "Epoch: [91 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0067 | top1:  99.9560 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0170 | top1:  76.7600 | top5:  93.7200\n",
            "\n",
            "Epoch: [92 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |################################| (391/391) Data: 0.002s | Batch: 0.095s | Total: 0:00:37 | ETA: 0:00:01 | Loss: 0.0065 | top1:  99.9680 | top5:  100.0000\n",
            "\u001b[KProcessing |################################| (100/100) Data: 0.003s | Batch: 0.029s | Total: 0:00:02 | ETA: 0:00:01 | Loss: 1.0058 | top1:  76.7400 | top5:  93.8600\n",
            "\n",
            "Epoch: [93 | 200] LR: 0.020000\n",
            "\u001b[KProcessing |#######                         | (94/391) Data: 0.004s | Batch: 0.097s | Total: 0:00:09 | ETA: 0:00:29 | Loss: 0.0066 | top1:  99.9584 | top5:  100.0000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUTKdnvy0RYI",
        "colab_type": "text"
      },
      "source": [
        "### 2. Evaluate trained models\n",
        "Evaluate trained (or pruned) models with **--evaluate** option and modify **--resume** accordingly to checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKN-sNmlyqX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python train.py -a wrn_16_8 -d cifar10 --resume checkpoints/model_best.pth.tar  --evaluate --save a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG9YqSuJq7Kr",
        "colab_type": "text"
      },
      "source": [
        "## 3. Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Pgru5b028z",
        "colab_type": "text"
      },
      "source": [
        "### 1. Filter Pruning\n",
        "\n",
        "For resnet56, resnet110, preresnet110, wrn_16_10: run following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feSBNuluQfnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python residualprune.py --dataset cifar100 --arch wrn_28_10 --model prune_2/model_best.pth.tar --save prune_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stoUkL4F0_Fo",
        "colab_type": "text"
      },
      "source": [
        "For vgg16: run following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIm6cwTUIEjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python vggprune.py --dataset cifar100 --arch vgg16 --model checkpoints/model_best.pth.tar --save prune_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70I3N2x31GXb",
        "colab_type": "text"
      },
      "source": [
        "### 2. Weight Pruning\n",
        "\n",
        "Run following command for **all** models.\n",
        "\n",
        "Note that to apply iterative pruning, we have to increase **precent** at each step manually. \n",
        "\n",
        "For example, in first pruning step, we set the --percent option to **0.2** and resume from baseline model; at second pruning step, we increase the --percent option to **0.4** and resume from (finetuned) pruned network of step 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ugGgHNSfgh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python cifar_prune.py --arch preresnet110 --dataset cifar10 --percent 0.96 --resume prune_2/finetuned.pth.tar   --save_dir prune_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSlOH2pKrFBQ",
        "colab_type": "text"
      },
      "source": [
        "## 4. Finetuning\n",
        "### 1. Finetune for Filter Pruning\n",
        "\n",
        "To retrain a network, which was pruned by **filter** pruning, run below command.\n",
        "\n",
        "For all models and datasets, we used $lr=0.1$, $schedule=[20,30]$, $gamma=0.1$. \n",
        "\n",
        "TODO: Finetune wideresnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvQ7jGK-QoV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python finetune.py --lr 0.02 --schedule 20 30 --gamma 0.2 --refine prune_3/pruned.pth.tar --dataset cifar100 --arch wrn_28_10 --save prune_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE7wK-XQ3MMY",
        "colab_type": "text"
      },
      "source": [
        "### 2. Finetune for Weight Pruning\n",
        "\n",
        "To retrain a network pruned by **weight** pruning, run following command.\n",
        "\n",
        "We used same hyperparameter as retraining for filter pruning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vXkO2d0QSOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python cifar_finetune.py --arch preresnet110 --dataset cifar10  --resume prune_3/pruned.pth.tar --save_dir prune_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSUJShf3rIZc",
        "colab_type": "text"
      },
      "source": [
        "## 4. Ensemble finetune\n",
        "\n",
        "To perform knowledge distillation from ensemble: manually change the **checkpoint_path** variable in *ensemble_finetune.py* then run below command.\n",
        "\n",
        "For knowledge distillation, we use $lr=0.01$, $schedule=[20,30]$, $gamma=0.2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td0xZSKOtZg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python ensemble_finetune.py --refine prune_3/finetuned.pth.tar --dataset cifar10 --save snapshot_ensemble --arch preresnet110"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znvcj-dpu4gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r prune_1.zip prune_1\n",
        "!zip -r prune_2.zip prune_2\n",
        "!zip -r prune_3.zip prune_3\n",
        "!zip -r prune_4.zip prune_4\n",
        "!zip -r prune_5.zip prune_5\n",
        "!zip -r kd.zip kd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS6tWKrtvAMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp kd.zip /content/drive/My\\ Drive/GINP/cifar10/weight_pruning/small_lr/resnet56_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG4tFNuDzSq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp prune_1.zip /content/drive/My\\ Drive/GINP/cifar10/weight_pruning/small_lr/resnet56_1\n",
        "!cp prune_2.zip /content/drive/My\\ Drive/GINP/cifar10/weight_pruning/small_lr/resnet56_1\n",
        "!cp prune_3.zip /content/drive/My\\ Drive/GINP/cifar10/weight_pruning/small_lr/resnet56_1\n",
        "!cp prune_4.zip /content/drive/My\\ Drive/GINP/cifar10/weight_pruning/small_lr/resnet56_1\n",
        "!cp prune_5.zip /content/drive/My\\ Drive/GINP/cifar10/weight_pruning/small_lr/resnet56_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjF_sqk6r09e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}